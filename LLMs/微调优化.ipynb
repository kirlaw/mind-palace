{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87a256f",
   "metadata": {},
   "source": [
    "## RLHF\n",
    "\n",
    "RLHF 是 “奖励建模 + 强化学习” 的范式，流程比较复杂：\n",
    "\n",
    "先收集人类对模型输出的偏好数据（A 比 B 好）。\n",
    "\n",
    "训练一个 Reward Model（奖励模型）来预测人类偏好。\n",
    "\n",
    "在大模型上做 PPO 或其他 RL 算法微调，让模型输出最大化奖励。\n",
    "\n",
    "适用场景：\n",
    "\n",
    "需要对齐复杂价值观时\n",
    "\n",
    "比如 ChatGPT 对话，涉及安全性、礼貌性、专业性。不同场景下「好」的定义可能很复杂，需要奖励模型来平衡。\n",
    "\n",
    "存在长尾或多维度偏好时\n",
    "\n",
    "例如新闻摘要，要既简洁又全面；客服助手，要既解决问题又保持礼貌。\n",
    "\n",
    "需要高度稳健性时\n",
    "\n",
    "比如大规模商用聊天机器人，RLHF 经过了很多实践验证。\n",
    "\n",
    "但 RLHF 成本高（需要奖励模型、PPO 训练、海量算力）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eefc40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
